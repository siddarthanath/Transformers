{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f036ed92",
   "metadata": {},
   "source": [
    "# Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637382ad",
   "metadata": {},
   "source": [
    "In this notebook. we are going to build a basic tokenizer, which will allow us to train a tokenizer and then test it, through encoding and decoding. We will be recreating a variant BPE algorithm, which has been widely used in OpenAI GPT models and other LLMs.\n",
    "\n",
    "**Note:** This is an implementation obtained from https://github.com/karpathy/minbpe - the legend Andrej Karpathy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac155e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from abc import abstractmethod, ABC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0fbe4e",
   "metadata": {},
   "source": [
    "## Base Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38ac3215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTokenizer(ABC):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # default: vocab size of 256 (all bytes), no merges, no patterns\n",
    "        self.merges = {} # (int, int) -> int\n",
    "        self.pattern = \"\" # str\n",
    "        self.special_tokens = {} # str -> int, e.g. {'<|endoftext|>': 100257}\n",
    "        self.vocab = self._build_vocab() # int -> bytes\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        # Tokenizer can train a vocabulary of size vocab_size from text\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Tokenizer can encode a string into a list of integers\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # Tokenizer can decode a list of integers into a string\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        # vocab is simply and deterministically derived from merges\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "        for (p0, p1), idx in self.merges.items():\n",
    "            vocab[idx] = vocab[p0] + vocab[p1]\n",
    "        for special, idx in self.special_tokens.items():\n",
    "            vocab[idx] = special.encode(\"utf-8\")\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf3cc827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stat(ids):\n",
    "    # Given a list of units, find the 2-gram tuple pairs and number of occurrences\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b2a8ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(pair, ids, idx):\n",
    "    # Given ids, we take the most occurring tuple pair and merge them\n",
    "    temp = []\n",
    "    # Loop through existing ids\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # If the ids match the maximum pair occurrence then replace it with new token index and move onto next pair\n",
    "        # Make sure that last token is not cut off\n",
    "        if i < len(ids) - 1 and pair == (ids[i], ids[i+1]):\n",
    "            temp.append(idx)\n",
    "            i += 2\n",
    "        # Else, add the first idx of the token and shift\n",
    "        else:\n",
    "            temp.append(ids[i])\n",
    "            i+=1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20df9408",
   "metadata": {},
   "source": [
    "## Basic Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448d107b",
   "metadata": {},
   "source": [
    "The tokenizer used in GPT is a modification of the original BPE algorithm. \n",
    "- The original BPE algorithm, developed for compressing data, merges the most frequent pair of bytes and replaces them with a new, unused byte, requiring a lookup table for decoding. \n",
    "- In contrast, the modified version used in large language models merges the most frequent pairs of characters (not bytes) into longer tokens without substituting them with unseen characters, thus eliminating the need for a lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "991c6859",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTokenizer(BaseTokenizer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def train(self, text, vocab_size):\n",
    "        # Check vocab size is larger than 256 (as UTF-8 has this minimum conversion size)\n",
    "        assert vocab_size >= 256\n",
    "        # Number of additional merges \n",
    "        num_merges = vocab_size - 255\n",
    "        # Change text to raw bytes\n",
    "        text_bytes = text.encode(\"utf-8\")\n",
    "        # Create list of integers in range 0 to 255\n",
    "        ids = list(text_bytes)\n",
    "        # Ieratively merge the most common pairs to create new tokens\n",
    "        merges = {} \n",
    "        # Store new tokens in vocab list\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)} \n",
    "        # Loop through number of merges\n",
    "        for i in range(num_merges):\n",
    "            print(f'Iteration {i+1} - text as ids: \\n{ids}')\n",
    "            # Obtain the pair occurrences\n",
    "            pairs = get_stat(ids)\n",
    "            # Find the pair with the maximum number of occurrences\n",
    "            try:\n",
    "                pair = max(pairs, key=lambda x: pairs[x])\n",
    "            except ValueError as v:\n",
    "                print('Number of preferred merges exceeds number of available merges!')\n",
    "                break\n",
    "            # Increment this to be a new token\n",
    "            idx = 256 + i\n",
    "            # Update the ids through merge\n",
    "            ids = merge(pair, ids, idx)\n",
    "            # Save the merged pair\n",
    "            merges[pair] = idx\n",
    "            print(f'Pair merged: {pair}\\n')\n",
    "            # Update the vocab list\n",
    "            vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "        # Save merges\n",
    "        self.merges = merges\n",
    "        # Save pairs\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def encode(self, text):\n",
    "        # Change text to raw bytes        \n",
    "        text_bytes = text.encode(\"utf-8\") \n",
    "        # Create list of integers in range 0 to 255\n",
    "        ids = list(text_bytes) \n",
    "        # Make sure text is at least length 2, otherwise there is no merge and we just return the byte\n",
    "        counter = 0\n",
    "        while len(ids) >= 2:\n",
    "            print(f'Iteration {counter+1} - text as ids: \\n{ids}')\n",
    "            counter += 1\n",
    "            # Get the occurrences in the text\n",
    "            pairs = get_stat(ids)\n",
    "            # Find the pair to merge from the merges dictionary - start from the recently added tokens and scale up\n",
    "            pair = min(pairs, key=lambda x: self.merges.get(x, float(\"inf\")))\n",
    "            # We must make sure we match the minimum pair to the keys in the merges otherwise we could get issues\n",
    "            if pair not in self.merges:\n",
    "                print(f'{pair} pair not in merge - merge process finished!')\n",
    "                break\n",
    "            # Else merge the lowest pair\n",
    "            print(f'Pair merged: {pair}\\n')\n",
    "            idx = self.merges[pair]\n",
    "            ids = merge(pair, ids, idx)\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        # Given the ids, obtain the bytes\n",
    "        text_bytes = b''.join(self.vocab[idx] for idx in ids)\n",
    "        # Decode the bytes\n",
    "        return text_bytes.decode('utf-8', errors='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8b46c3",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07fa5258",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = 'Hi there! What are you doing? Do you know what the weather is like today? If you do, where would you go?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2a36d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = BasicTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f6282c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - text as ids: \n",
      "[72, 105, 32, 116, 104, 101, 114, 101, 33, 32, 87, 104, 97, 116, 32, 97, 114, 101, 32, 121, 111, 117, 32, 100, 111, 105, 110, 103, 63, 32, 68, 111, 32, 121, 111, 117, 32, 107, 110, 111, 119, 32, 119, 104, 97, 116, 32, 116, 104, 101, 32, 119, 101, 97, 116, 104, 101, 114, 32, 105, 115, 32, 108, 105, 107, 101, 32, 116, 111, 100, 97, 121, 63, 32, 73, 102, 32, 121, 111, 117, 32, 100, 111, 44, 32, 119, 104, 101, 114, 101, 32, 119, 111, 117, 108, 100, 32, 121, 111, 117, 32, 103, 111, 63]\n",
      "Pair merged: (111, 117)\n",
      "\n",
      "Iteration 2 - text as ids: \n",
      "[72, 105, 32, 116, 104, 101, 114, 101, 33, 32, 87, 104, 97, 116, 32, 97, 114, 101, 32, 121, 256, 32, 100, 111, 105, 110, 103, 63, 32, 68, 111, 32, 121, 256, 32, 107, 110, 111, 119, 32, 119, 104, 97, 116, 32, 116, 104, 101, 32, 119, 101, 97, 116, 104, 101, 114, 32, 105, 115, 32, 108, 105, 107, 101, 32, 116, 111, 100, 97, 121, 63, 32, 73, 102, 32, 121, 256, 32, 100, 111, 44, 32, 119, 104, 101, 114, 101, 32, 119, 256, 108, 100, 32, 121, 256, 32, 103, 111, 63]\n",
      "Pair merged: (104, 101)\n",
      "\n",
      "Iteration 3 - text as ids: \n",
      "[72, 105, 32, 116, 257, 114, 101, 33, 32, 87, 104, 97, 116, 32, 97, 114, 101, 32, 121, 256, 32, 100, 111, 105, 110, 103, 63, 32, 68, 111, 32, 121, 256, 32, 107, 110, 111, 119, 32, 119, 104, 97, 116, 32, 116, 257, 32, 119, 101, 97, 116, 257, 114, 32, 105, 115, 32, 108, 105, 107, 101, 32, 116, 111, 100, 97, 121, 63, 32, 73, 102, 32, 121, 256, 32, 100, 111, 44, 32, 119, 257, 114, 101, 32, 119, 256, 108, 100, 32, 121, 256, 32, 103, 111, 63]\n",
      "Pair merged: (32, 121)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tk.train(train_text, vocab_size=258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdd24d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(111, 117): 256, (104, 101): 257, (32, 121): 258}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd735472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b'ou',\n",
       " 257: b'he',\n",
       " 258: b' y'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed53258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = 'Hi there! You look amazing today. You should go out!'Neural Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dc5c919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 - text as ids: \n",
      "[72, 105, 32, 116, 104, 101, 114, 101, 33, 32, 89, 111, 117, 32, 108, 111, 111, 107, 32, 97, 109, 97, 122, 105, 110, 103, 32, 116, 111, 100, 97, 121, 46, 32, 89, 111, 117, 32, 115, 104, 111, 117, 108, 100, 32, 103, 111, 32, 111, 117, 116, 33]\n",
      "Pair merged: (111, 117)\n",
      "\n",
      "Iteration 2 - text as ids: \n",
      "[72, 105, 32, 116, 104, 101, 114, 101, 33, 32, 89, 256, 32, 108, 111, 111, 107, 32, 97, 109, 97, 122, 105, 110, 103, 32, 116, 111, 100, 97, 121, 46, 32, 89, 256, 32, 115, 104, 256, 108, 100, 32, 103, 111, 32, 256, 116, 33]\n",
      "Pair merged: (104, 101)\n",
      "\n",
      "Iteration 3 - text as ids: \n",
      "[72, 105, 32, 116, 257, 114, 101, 33, 32, 89, 256, 32, 108, 111, 111, 107, 32, 97, 109, 97, 122, 105, 110, 103, 32, 116, 111, 100, 97, 121, 46, 32, 89, 256, 32, 115, 104, 256, 108, 100, 32, 103, 111, 32, 256, 116, 33]\n",
      "(72, 105) pair not in merge - merge process finished!\n"
     ]
    }
   ],
   "source": [
    "test_ids = tk.encode(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60896fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_decoded_text = tk.decode(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b1c863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_decoded_text == test_text, 'Text through encoding and decoding via tokenisation is NOT the same as the original text!'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7798d40d",
   "metadata": {},
   "source": [
    "Hooray!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
